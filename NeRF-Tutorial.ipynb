{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kazuwjn/nerf-colab/blob/main/NeRF-Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IDbsWo1ysJK"
      },
      "source": [
        "# NeRF (Neural Radiance Fields)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvbXRDTm8oDv"
      },
      "source": [
        "このノートブックでは、 NeRF (https://www.matthewtancik.com/nerf) を誰でも簡単に試せるように\n",
        "\n",
        "* データセットの作成\n",
        "* NeRFの学習\n",
        "* 実験結果の可視化\n",
        "\n",
        "ができるようになっています。\n",
        "\n",
        "NeRFについて知りたい場合は以下のサイトなどを参考にしてください。\n",
        "\n",
        "* https://ai-scholar.tech/articles/nerf/nerf\n",
        "* https://www.slideshare.net/DeepLearningJP2016/dlnerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis\n",
        "* https://blog.albert2005.co.jp/2020/05/08/nerf/\n",
        "\n",
        "応物ゼミ第2回のスライドです。簡単にNeRFとその前提知識についてまとめてあります。\n",
        "\n",
        "https://drive.google.com/drive/folders/17vv_BQATx0_5nS9QuJ6XGWHPUJzTgng9?usp=sharing\n",
        "\n",
        "<font color=\"FireBrick\">注意: 学習時間の短縮のため、このノートブックでは torch-ngp (https://github.com/ashawkey/torch-ngp) を使用しています。</font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whlQn2CkzXnc"
      },
      "source": [
        "## データセット作成\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2q7jHrv8h5H"
      },
      "source": [
        "動画または複数枚の画像からカメラの内部パラメータと外部パラメータを推定してデータセットを作成します。\n",
        "\n",
        "\n",
        "すでにデータセットが作成できている場合は [NeRFの学習] に進んでください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SudOJDKH2W21"
      },
      "source": [
        "### 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEPN_6cSuDUP"
      },
      "source": [
        "1. Google アカウントをお持ちでない場合は Google アカウントを作成してください。\n",
        "    \n",
        "    https://accounts.google.com/signup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMfYnuD1uEro"
      },
      "source": [
        "2. Google Drive をマウントし、必要なディレクトリを作成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BDnUIUYuED4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p \"/content/drive/MyDrive/NeRF-Tutorial/datasets\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQzopk7szmLI"
      },
      "source": [
        "3. NeRFで復元したいオブジェクトを撮影してください。\n",
        "\n",
        "    動画だとモーションブラーが発生しやすいので、画像をおすすめします。\n",
        "    \n",
        "    <details>\n",
        "    <summary>注意: iPhoneで撮影する場合</summary>\n",
        "\n",
        "    設定 > カメラ > フォーマット > カメラ撮影を「互換性優先」にしてください。\n",
        "    <img src=\"http://drive.google.com/uc?export=view&id=1T_8VTS4REQyZ1gh4k7i91wtpwkXoPv5e\" width=\"75%\">\n",
        "    </details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax9mhADx8IJ4"
      },
      "source": [
        "4. 撮影した動画または複数枚の画像を Google Drive にアップロードしてください。\n",
        "    \n",
        "    スマートフォンで撮影した場合は Google Drive アプリ経由でアップロードすると良いです。\n",
        "    \n",
        "    <table style=\"border:none;\">\n",
        "    <tbody style=\"border:none;\">\n",
        "        <tr style=\"border:none;\">\n",
        "        <td style=\"border:none;\" align=\"center\">App Store</td>\n",
        "        <td style=\"border:none;\" align=\"center\">Google Play Store</td>\n",
        "        </tr>\n",
        "        <tr style=\"border:none;\">\n",
        "        <td style=\"border:none;\" align=\"center\"><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAARgAAAEYAQMAAAC9QHvPAAAABlBMVEX///8AAABVwtN+AAAACXBIWXMAAA7EAAAOxAGVKw4bAAABDElEQVRoge2VSw4CMQxDff9LG6lxPoVhOavYiFDSNxvLyQCWZb0phsBzwqjnYKbrIVHfrNsZOYi0sRw1M5ljneJm5pkBzPwwqnFzfthdM73D5+ffnl/HlNLDh6vNTPQiVWKCnzO4mqlvrHE9prMZVLCYz2Bu9eVMtuMw2+3hcgY5iSNYXxauZnpLneHLuF2OrmWoCYy9FGNoZnpYN9PEBNczRLdlWqSMZtSBLGMZGq84M8ggVWXSl4WLmQRy5sjrr5kYwrPIVVNmppHq5pvuidrItF/FyNMO2WqmaoQM5aEZDM9qcTPzRTODUUdpCw9pZjCcl5U1MxdZ8/c0pysZ1sRpmUepx5czlmW9ow/iJfO1xBytWAAAAABJRU5ErkJggg==\" width=\"150vw\"></td>\n",
        "        <td style=\"border:none;\" align=\"center\"><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAWAAAAFgAQMAAAC/rzEPAAAABlBMVEX///8AAABVwtN+AAAACXBIWXMAAA7EAAAOxAGVKw4bAAAB9UlEQVRoge2XQZbDMAhDfQPuf0tu4GmDhMm0WcRkNSPympfWnywsI+gYCoVC8T/C5hFu/np8f163ePKyKrgNO+4WP2VyZPliBPdgC0neTxDGcU1fq4KfgmPzDzEMmgzBj8PQ5H3snYmXogi+D4+yNEhaEFeOJPg2nE3z63XVYQXfhBlG717FMC5D8G34WHX6iyMLA6F/HH7BuzC85ODQL0OX89gt+Al4UhQOJSyEsBkX3IZnChLzXy6tN1ATwfvwUmVwAowv8QPEEdyFsRI2Q2XC0lEGZTYRvA2PMgXG4WenzBmlKih4E85NN4wmPPfOqrBlM4K3YTgJLd2zBJYkcwpuw8YFPjA3Mqsiglswtj0P/zkZjOAmXHKogKXpMF1wGw4qrNvYPiEOZClNU/AuHCjug6LEnILmWYdAwQ042FlOe4qBVcFteHDnqUH6d6kHwW04pxNksV/6SPcxwW2Yp9tm0QP8hCq/C0XwBryG61SFo3Y4TaEF78PslilMenhajAl+AOb2o0kuZQI9V4rgTTiT4ClpN0stwQ/Axn2vKsB4LC/BbZiNkpMJOiZNnRUhuAvDwvkHZsLNl1yCn4TzvM+sAsixXEZwGx7wk0ByYLGvogjegJniOaIYiyFfJrgNs0c6xpNh69Tj9tFhBd+GFQqF4q/HD1A9a7z+Q0x5AAAAAElFTkSuQmCC\" width=\"150vw\"></td>\n",
        "        </tr>\n",
        "    </tbody>\n",
        "    </table>\n",
        "\n",
        "    撮影した動画または画像は以下のフォルダに保存してください。\n",
        "\n",
        "    ```\n",
        "    MyDrive/NeRF-Tutorial/datasets/{適当なデータセットの名前}\n",
        "    e.g. MyDrive/NeRF-Tutorial/datasets/miku\n",
        "    ```\n",
        "\n",
        "    <details>\n",
        "    <summary>アップロードの仕方</summary>\n",
        "\n",
        "    <img src=\"http://drive.google.com/uc?export=view&id=1U7td9vhnZAEjFCIyT9lHIY20HZLxOCnZ\" width=\"30%\">\n",
        "    <img src=\"http://drive.google.com/uc?export=view&id=1dyOfhiiopBT7i7Reh8dnpfTv7OFiQGRU\" width=\"30%\">\n",
        "    <img src=\"http://drive.google.com/uc?export=view&id=1lCivrC4ZoTLpMYOpSQOvjQAyWeXYW1Rw\" width=\"30%\">\n",
        "    <img src=\"http://drive.google.com/uc?export=view&id=1HGZSWXaYRz8mQAmBC3Oq42BBMpC8ENXK\" width=\"30%\">\n",
        "    <img src=\"http://drive.google.com/uc?export=view&id=1oUwX57tProLqSD4wWY2hEH5ZKzBpb6kd\" width=\"30%\">\n",
        "    <img src=\"http://drive.google.com/uc?export=view&id=1oAdFXsTgvPS2thUn6dterSX_BoWYAjq-\" width=\"30%\">\n",
        "    <img src=\"http://drive.google.com/uc?export=view&id=1NT7lmRw79QEI06D_3io1TtXrWJ4TR4al\" width=\"30%\">\n",
        "    \n",
        "    </details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED9WqMBjIntb"
      },
      "source": [
        "### 前処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3puAvM1nJL3z"
      },
      "source": [
        "1. Google Drive をマウントします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUHIknEbydpA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTTP11c8P_Rs"
      },
      "source": [
        "2. 撮影した動画または画像が保存されているフォルダパスを入力してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADdUc4yIJ5cg"
      },
      "outputs": [],
      "source": [
        "#@title { display-mode: \"form\" }\n",
        "#@markdown ←コード実行\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "media_path = \"/content/drive/MyDrive/NeRF-Tutorial/datasets/miku\" # @param {type: 'string'}\n",
        "media_path = Path(media_path)\n",
        "\n",
        "images_dir = media_path / 'images'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQLIE85AIr0y"
      },
      "source": [
        "3. 動画の場合は ffmpeg を使って連番画像に変換します。\n",
        "\n",
        "    動画から切り出す画像の枚数を入力してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFjx803vJ1RM"
      },
      "outputs": [],
      "source": [
        "#@title { display-mode: \"form\" }\n",
        "#@markdown ←コード実行\n",
        "\n",
        "import cv2\n",
        "\n",
        "video_ext = ['*.mp4', '*.MP4', '*.mov', '*.MOV', '*.avi', '*.AVI']\n",
        "\n",
        "video_paths = []\n",
        "for ext in video_ext:\n",
        "    video_paths.extend(media_path.glob(ext))\n",
        "\n",
        "if len(video_paths) == 0:\n",
        "    raise RuntimeError('動画ファイルが存在しません。（対応拡張子: mp4, mov, avi）')\n",
        "\n",
        "video_path = video_paths[0]\n",
        "\n",
        "target_num_frames = 100 # @param {type: 'number'}\n",
        "\n",
        "cap = cv2.VideoCapture(str(video_path))\n",
        "input_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "if num_frames < target_num_frames:\n",
        "  raise RuntimeError(f'動画のフレーム数がtarget_num_frames({target_num_frames})よりも少ない。')\n",
        "\n",
        "fps = -(-target_num_frames * input_fps) // num_frames\n",
        "print(f\"FPS = {fps}\")\n",
        "\n",
        "tmp_images_dir = 'images/'\n",
        "out_pattern = str('images/%04d.png')\n",
        "!mkdir -p \"$tmp_images_dir\"\n",
        "!ffmpeg -i \"$video_path\" -r $fps \"$out_pattern\"\n",
        "\n",
        "images_dir = media_path\n",
        "!mkdir -p \"$images_dir\"\n",
        "!rsync -av \"$tmp_images_dir/\" \"$images_dir/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCC12GLpKm1_"
      },
      "source": [
        "4. 画像の解像度を 600×600 以下になるように下げます。\n",
        "\n",
        "    これは [COLMAPによるカメラパラメータ推定] の高速化のために行います。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iopdHpCRKnsD"
      },
      "outputs": [],
      "source": [
        "#@title { display-mode: \"form\" }\n",
        "#@markdown ←コード実行\n",
        "\n",
        "import imageio\n",
        "import cv2\n",
        "\n",
        "image_ext = ['*.png', '*.PNG', '*.jpg', '*.JPG', '*.jpeg', '*.JPEG']\n",
        "\n",
        "image_paths = []\n",
        "for ext in image_ext:\n",
        "    image_paths.extend(media_path.glob(ext))\n",
        "\n",
        "if len(image_paths) == 0:\n",
        "    raise RuntimeError('画像ファイルが存在しません。（対応拡張子: png, jpg）')\n",
        "\n",
        "image = imageio.imread(image_paths[0])\n",
        "height, width = image.shape[:2]\n",
        "resolution = height * width\n",
        "scale = 1\n",
        "while resolution > 600 * 600:\n",
        "    scale <<= 1\n",
        "    resolution >>= 2\n",
        "\n",
        "images_dir = media_path / 'images'\n",
        "images_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "for image_path in image_paths:\n",
        "  print(f'Done: {image_path}')\n",
        "  image = imageio.imread(image_path)\n",
        "  height, width = image.shape[:2]\n",
        "  out_height, out_width = height // scale, width // scale\n",
        "  resized = cv2.resize(image, (out_width, out_height), cv2.INTER_AREA)\n",
        "  resized_path = images_dir / f'{image_path.stem}.png'\n",
        "  imageio.imwrite(resized_path, resized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xwf_o207xnv"
      },
      "source": [
        "### COLMAPによるカメラパラメータ推定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy0hrXrpCrK0"
      },
      "source": [
        "COLMAPという SfM (Structure from Motion) のツールを使ってカメラの内部パラメータ及び外部パラメータを推定します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkMZJhIUDTG2"
      },
      "outputs": [],
      "source": [
        "!apt install colmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GQ-lFJB6yvF_"
      },
      "outputs": [],
      "source": [
        "#@title { display-mode: \"form\" }\n",
        "#@markdown ←コード実行\n",
        "\n",
        "#@markdown COLMAP でカメラパラメータを推定し、データセットを作成します。\n",
        "\n",
        "#@markdown 画像解像度で処理時間が大きく変わりますが、だいたい30分ほどかかります。 \n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "import sys\n",
        "import math\n",
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def run_colmap(args):\n",
        "    db_path = args.colmap_db\n",
        "    images_dir = args.images_dir\n",
        "    text_dir = args.colmap_text\n",
        "    flag_EAS = int(args.estimate_affine_shape) # 0 / 1\n",
        "\n",
        "    sparse_dir = args.root_dir / \"colmap_sparse\"\n",
        "\n",
        "    print(f\"running colmap with:\\n\\tdb={db_path}\\n\\timages={images_dir}\\n\\tsparse={sparse_dir}\\n\\ttext={text_dir}\")\n",
        "    if os.path.exists(db_path):\n",
        "        os.remove(db_path)\n",
        "    \n",
        "    !colmap feature_extractor --SiftExtraction.use_gpu 0  --ImageReader.camera_model OPENCV --ImageReader.single_camera 1 --database_path \"{db_path}\" --image_path \"{images_dir}\"\n",
        "    !colmap {args.colmap_matcher}_matcher --SiftMatching.use_gpu 0 --database_path \"{db_path}\"\n",
        "    # !wget https://demuc.de/colmap/vocab_tree_flickr100K_words32K.bin\n",
        "    # !colmap vocab_tree_matcher --VocabTreeMatching.vocab_tree_path vocab_tree_flickr100K_words32K.bin --SiftMatching.use_gpu 0 --database_path \"{db_path}\"\n",
        "    try:\n",
        "        shutil.rmtree(sparse_dir)\n",
        "    except:\n",
        "        pass\n",
        "    !mkdir \"{sparse_dir}\"\n",
        "    !colmap mapper --database_path \"{db_path}\" --image_path \"{images_dir}\" --export_path \"{sparse_dir}\"\n",
        "    !colmap bundle_adjuster --input_path \"{sparse_dir}/0\" --output_path \"{sparse_dir}/0\" --BundleAdjustment.refine_principal_point 1\n",
        "    try:\n",
        "        shutil.rmtree(text_dir)\n",
        "    except:\n",
        "        pass\n",
        "    !mkdir \"{text_dir}\"\n",
        "    !colmap model_converter --input_path \"{sparse_dir}/0\" --output_path \"{text_dir}\" --output_type TXT\n",
        "\n",
        "def variance_of_laplacian(image):\n",
        "    return cv2.Laplacian(image, cv2.CV_64F).var()\n",
        "\n",
        "def sharpness(imagePath):\n",
        "    image = cv2.imread(imagePath)\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    fm = variance_of_laplacian(gray)\n",
        "    return fm\n",
        "\n",
        "def qvec2rotmat(qvec):\n",
        "    return np.array([\n",
        "        [\n",
        "            1 - 2 * qvec[2]**2 - 2 * qvec[3]**2,\n",
        "            2 * qvec[1] * qvec[2] - 2 * qvec[0] * qvec[3],\n",
        "            2 * qvec[3] * qvec[1] + 2 * qvec[0] * qvec[2]\n",
        "        ], [\n",
        "            2 * qvec[1] * qvec[2] + 2 * qvec[0] * qvec[3],\n",
        "            1 - 2 * qvec[1]**2 - 2 * qvec[3]**2,\n",
        "            2 * qvec[2] * qvec[3] - 2 * qvec[0] * qvec[1]\n",
        "        ], [\n",
        "            2 * qvec[3] * qvec[1] - 2 * qvec[0] * qvec[2],\n",
        "            2 * qvec[2] * qvec[3] + 2 * qvec[0] * qvec[1],\n",
        "            1 - 2 * qvec[1]**2 - 2 * qvec[2]**2\n",
        "        ]\n",
        "    ])\n",
        "\n",
        "def rotmat(a, b):\n",
        "\ta, b = a / np.linalg.norm(a), b / np.linalg.norm(b)\n",
        "\tv = np.cross(a, b)\n",
        "\tc = np.dot(a, b)\n",
        "\t# handle exception for the opposite direction input\n",
        "\tif c < -1 + 1e-10:\n",
        "\t\treturn rotmat(a + np.random.uniform(-1e-2, 1e-2, 3), b)\n",
        "\ts = np.linalg.norm(v)\n",
        "\tkmat = np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])\n",
        "\treturn np.eye(3) + kmat + kmat.dot(kmat) * ((1 - c) / (s ** 2 + 1e-10))\n",
        "\n",
        "def closest_point_2_lines(oa, da, ob, db): # returns point closest to both rays of form o+t*d, and a weight factor that goes to 0 if the lines are parallel\n",
        "    da = da / np.linalg.norm(da)\n",
        "    db = db / np.linalg.norm(db)\n",
        "    c = np.cross(da, db)\n",
        "    denom = np.linalg.norm(c)**2\n",
        "    t = ob - oa\n",
        "    ta = np.linalg.det([t, db, c]) / (denom + 1e-10)\n",
        "    tb = np.linalg.det([t, da, c]) / (denom + 1e-10)\n",
        "    if ta > 0:\n",
        "        ta = 0\n",
        "    if tb > 0:\n",
        "        tb = 0\n",
        "    return (oa+ta*da+ob+tb*db) * 0.5, denom\n",
        "\n",
        "class Arg():\n",
        "    def __init__(self):\n",
        "        self.root_dir = images_dir.parent\n",
        "        self.images_dir = images_dir\n",
        "        self.run_colmap = True\n",
        "        self.dynamic = False\n",
        "        self.estimate_affine_shape = False\n",
        "        self.hold = 8\n",
        "        self.video_fps = 3\n",
        "        self.time_slice = \"\"\n",
        "        self.colmap_matcher = \"exhaustive\"\n",
        "        self.skip_early = 0\n",
        "        self.colmap_db = \"colmap.db\"\n",
        "        self.colmap_text = \"colmap_text\"\n",
        "\n",
        "args = Arg()\n",
        "\n",
        "args.colmap_db = args.root_dir / args.colmap_db\n",
        "args.colmap_text = args.root_dir / args.colmap_text\n",
        "\n",
        "if args.run_colmap:\n",
        "    run_colmap(args)\n",
        "\n",
        "SKIP_EARLY = int(args.skip_early)\n",
        "TEXT_FOLDER = args.colmap_text\n",
        "\n",
        "with open(os.path.join(TEXT_FOLDER, \"cameras.txt\"), \"r\") as f:\n",
        "    angle_x = math.pi / 2\n",
        "    for line in f:\n",
        "        # 1 SIMPLE_RADIAL 2048 1536 1580.46 1024 768 0.0045691\n",
        "        # 1 OPENCV 3840 2160 3178.27 3182.09 1920 1080 0.159668 -0.231286 -0.00123982 0.00272224\n",
        "        # 1 RADIAL 1920 1080 1665.1 960 540 0.0672856 -0.0761443\n",
        "        if line[0] == \"#\":\n",
        "            continue\n",
        "        els = line.split(\" \")\n",
        "        w = float(els[2])\n",
        "        h = float(els[3])\n",
        "        fl_x = float(els[4])\n",
        "        fl_y = float(els[4])\n",
        "        k1 = 0\n",
        "        k2 = 0\n",
        "        p1 = 0\n",
        "        p2 = 0\n",
        "        cx = w / 2\n",
        "        cy = h / 2\n",
        "        if els[1] == \"SIMPLE_PINHOLE\":\n",
        "            cx = float(els[5])\n",
        "            cy = float(els[6])\n",
        "        elif els[1] == \"PINHOLE\":\n",
        "            fl_y = float(els[5])\n",
        "            cx = float(els[6])\n",
        "            cy = float(els[7])\n",
        "        elif els[1] == \"SIMPLE_RADIAL\":\n",
        "            cx = float(els[5])\n",
        "            cy = float(els[6])\n",
        "            k1 = float(els[7])\n",
        "        elif els[1] == \"RADIAL\":\n",
        "            cx = float(els[5])\n",
        "            cy = float(els[6])\n",
        "            k1 = float(els[7])\n",
        "            k2 = float(els[8])\n",
        "        elif els[1] == \"OPENCV\":\n",
        "            fl_y = float(els[5])\n",
        "            cx = float(els[6])\n",
        "            cy = float(els[7])\n",
        "            k1 = float(els[8])\n",
        "            k2 = float(els[9])\n",
        "            p1 = float(els[10])\n",
        "            p2 = float(els[11])\n",
        "        else:\n",
        "            print(\"unknown camera model \", els[1])\n",
        "        # fl = 0.5 * w / tan(0.5 * angle_x);\n",
        "        angle_x = math.atan(w / (fl_x * 2)) * 2\n",
        "        angle_y = math.atan(h / (fl_y * 2)) * 2\n",
        "        fovx = angle_x * 180 / math.pi\n",
        "        fovy = angle_y * 180 / math.pi\n",
        "\n",
        "print(f\"camera:\\n\\tres={w,h}\\n\\tcenter={cx,cy}\\n\\tfocal={fl_x,fl_y}\\n\\tfov={fovx,fovy}\\n\\tk={k1,k2} p={p1,p2} \")\n",
        "\n",
        "with open(os.path.join(TEXT_FOLDER, \"images.txt\"), \"r\") as f:\n",
        "    i = 0\n",
        "\n",
        "    bottom = np.array([0.0, 0.0, 0.0, 1.0]).reshape([1, 4])\n",
        "\n",
        "    frames = []\n",
        "\n",
        "    up = np.zeros(3)\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "\n",
        "        if line[0] == \"#\":\n",
        "            continue\n",
        "\n",
        "        i = i + 1\n",
        "        if i < SKIP_EARLY*2:\n",
        "            continue\n",
        "\n",
        "        if i % 2 == 1:\n",
        "            elems = line.split(\" \") # 1-4 is quat, 5-7 is trans, 9ff is filename (9, if filename contains no spaces)\n",
        "\n",
        "            name = '_'.join(elems[9:])\n",
        "            full_name = str(args.images_dir / name)\n",
        "            rel_name = f'images/{name}'\n",
        "\n",
        "            b = sharpness(full_name)\n",
        "            # print(name, \"sharpness =\",b)\n",
        "\n",
        "            image_id = int(elems[0])\n",
        "            qvec = np.array(tuple(map(float, elems[1:5])))\n",
        "            tvec = np.array(tuple(map(float, elems[5:8])))\n",
        "            R = qvec2rotmat(-qvec)\n",
        "            t = tvec.reshape([3, 1])\n",
        "            m = np.concatenate([np.concatenate([R, t], 1), bottom], 0)\n",
        "            c2w = np.linalg.inv(m)\n",
        "            \n",
        "            c2w[0:3, 2] *= -1 # flip the y and z axis\n",
        "            c2w[0:3, 1] *= -1\n",
        "            c2w = c2w[[1, 0, 2, 3],:] # swap y and z\n",
        "            c2w[2, :] *= -1 # flip whole world upside down\n",
        "\n",
        "            up += c2w[0:3, 1]\n",
        "\n",
        "            frame = {\n",
        "                \"file_path\": rel_name, \n",
        "                \"sharpness\": b, \n",
        "                \"transform_matrix\": c2w\n",
        "            }\n",
        "\n",
        "            frames.append(frame)\n",
        "\n",
        "N = len(frames)\n",
        "up = up / np.linalg.norm(up)\n",
        "\n",
        "print(\"[INFO] up vector was\", up)\n",
        "\n",
        "R = rotmat(up, [0, 0, 1]) # rotate up vector to [0,0,1]\n",
        "R = np.pad(R, [0, 1])\n",
        "R[-1, -1] = 1\n",
        "\n",
        "for f in frames:\n",
        "    f[\"transform_matrix\"] = np.matmul(R, f[\"transform_matrix\"]) # rotate up to be the z axis\n",
        "\n",
        "# find a central point they are all looking at\n",
        "print(\"[INFO] computing center of attention...\")\n",
        "totw = 0.0\n",
        "totp = np.array([0.0, 0.0, 0.0])\n",
        "for f in frames:\n",
        "    mf = f[\"transform_matrix\"][0:3,:]\n",
        "    for g in frames:\n",
        "        mg = g[\"transform_matrix\"][0:3,:]\n",
        "        p, weight = closest_point_2_lines(mf[:,3], mf[:,2], mg[:,3], mg[:,2])\n",
        "        if weight > 0.01:\n",
        "            totp += p * weight\n",
        "            totw += weight\n",
        "totp /= totw\n",
        "for f in frames:\n",
        "    f[\"transform_matrix\"][0:3,3] -= totp\n",
        "avglen = 0.\n",
        "for f in frames:\n",
        "    avglen += np.linalg.norm(f[\"transform_matrix\"][0:3,3])\n",
        "avglen /= N\n",
        "print(\"[INFO] avg camera distance from origin\", avglen)\n",
        "for f in frames:\n",
        "    f[\"transform_matrix\"][0:3,3] *= 4.0 / avglen # scale to \"nerf sized\"\n",
        "\n",
        "# sort frames by id\n",
        "frames.sort(key=lambda d: d['file_path'])\n",
        "\n",
        "# add time if scene is dynamic\n",
        "if args.dynamic:\n",
        "    for i, f in enumerate(frames):\n",
        "        f['time'] = i / N\n",
        "\n",
        "for f in frames:\n",
        "    f[\"transform_matrix\"] = f[\"transform_matrix\"].tolist()\n",
        "\n",
        "# construct frames\n",
        "\n",
        "def write_json(filename, frames):\n",
        "\n",
        "    out = {\n",
        "        \"camera_angle_x\": angle_x,\n",
        "        \"camera_angle_y\": angle_y,\n",
        "        \"fl_x\": fl_x,\n",
        "        \"fl_y\": fl_y,\n",
        "        \"k1\": k1,\n",
        "        \"k2\": k2,\n",
        "        \"p1\": p1,\n",
        "        \"p2\": p2,\n",
        "        \"cx\": cx,\n",
        "        \"cy\": cy,\n",
        "        \"w\": w,\n",
        "        \"h\": h,\n",
        "        \"frames\": frames,\n",
        "    }\n",
        "\n",
        "    output_path = args.root_dir / filename\n",
        "    print(f\"[INFO] writing {len(frames)} frames to {output_path}\")\n",
        "    with open(output_path, \"w\") as outfile:\n",
        "        json.dump(out, outfile, indent=2)\n",
        "\n",
        "# just one transforms.json, don't do data split\n",
        "if args.hold <= 0:\n",
        "\n",
        "    write_json('transforms.json', frames)\n",
        "    \n",
        "else:\n",
        "    all_ids = np.arange(N)\n",
        "    test_ids = all_ids[::args.hold]\n",
        "    train_ids = np.array([i for i in all_ids if i not in test_ids])\n",
        "\n",
        "    frames_train = [f for i, f in enumerate(frames) if i in train_ids]\n",
        "    frames_test = [f for i, f in enumerate(frames) if i in test_ids]\n",
        "\n",
        "    write_json('transforms_train.json', frames_train)\n",
        "    write_json('transforms_val.json', frames_test[::10])\n",
        "    write_json('transforms_test.json', frames_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwdQNdHr6b4Z"
      },
      "source": [
        "## NeRFの学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWWhym5uSzJf"
      },
      "source": [
        "このノートブックでは、NeRFの学習時間の短縮のために [torch-ngp](https://github.com/ashawkey/torch-ngp) を使います。\n",
        "\n",
        "通常のNeRFでは学習時間が10時間以上かかりますが、 torch-ngp では1時間以内に学習が終わります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQI_PukbiGrK"
      },
      "source": [
        "1. Google Drive をマウントします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFnEO6UjiHEq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADx1u6dPWFvC"
      },
      "source": [
        "2. 必要なコードとモジュールをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jE-xpP226rcG"
      },
      "outputs": [],
      "source": [
        "!git clone --recursive https://github.com/ashawkey/torch-ngp.git\n",
        "%cd /content/torch-ngp/\n",
        "!pip install -r requirements.txt\n",
        "!pip install imageio-ffmpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNLofxOvWQWp"
      },
      "source": [
        "3. データセットが保存されているフォルダパスを入力してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BwB72bKWZoN"
      },
      "outputs": [],
      "source": [
        "#@title { display-mode: \"form\" }\n",
        "#@markdown ←コード実行\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/NeRF-Tutorial/datasets/miku\" # @param {type: 'string'}\n",
        "dataset_path = Path(dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVjkv0wsWkC0"
      },
      "source": [
        "4. NeRF (torch-ngp) を学習します。\n",
        "\n",
        "    学習結果は `MyDrive/NeRF-Tutorial/experiments/exp_{日付}` に保存されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUIor0Az6rzL"
      },
      "outputs": [],
      "source": [
        "#@title { display-mode: \"form\" }\n",
        "#@markdown ←コード実行\n",
        "\n",
        "import datetime\n",
        "now_jst = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9))).strftime('%m%d%H%M')\n",
        "\n",
        "scale = 1 #@param {type:\"slider\", min:0.1, max:2, step:0.1}\n",
        "bound = 8 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "bg_radius = 32 #@param {type:\"slider\", min:16, max:64, step:16}\n",
        "iters = 25000 #@param {type:\"slider\", min:20000, max:30000, step:1000}\n",
        "\n",
        "!python main_nerf.py \"{dataset_path}\" --workspace \"exp_{now_jst}\" --fp16 --cuda_ray --scale {scale} --bound {bound} --dt_gamma 0.00 --bg_radius {bg_radius} --iters {iters}\n",
        "!mkdir -p \"/content/drive/MyDrive/NeRF-Tutorial/experiments/exp_{now_jst}\"\n",
        "!rsync -av \"/content/torch-ngp/exp_{now_jst}/\" \"/content/drive/MyDrive/NeRF-Tutorial/experiments/exp_{now_jst}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGErl3NOnXJT"
      },
      "source": [
        "## 実験結果の可視化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOZ5tnES5Zrl"
      },
      "source": [
        "1. Google Drive をマウントします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZL3C7v2kPhl1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7iLNXsy5jNz"
      },
      "source": [
        "2. 必要なコードとモジュールをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54oolmCszJl9"
      },
      "outputs": [],
      "source": [
        "!git clone --recursive https://github.com/ashawkey/torch-ngp.git\n",
        "%cd /content/torch-ngp/\n",
        "!pip install -r requirements.txt\n",
        "!pip install imageio-ffmpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PaGwx0W5s_A"
      },
      "source": [
        "3. データセットが保存されているフォルダパスと学習結果が保存されているフォルダパスを入力してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCsxAEZEPIFj"
      },
      "outputs": [],
      "source": [
        "#@title { display-mode: \"form\" }\n",
        "#@markdown ←コード実行\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/NeRF-Tutorial/datasets/miku\" # @param {type: 'string'}\n",
        "dataset_path = Path(dataset_path)\n",
        "exp_path = \"/content/drive/MyDrive/NeRF-Tutorial/experiments/exp_08302153\" # @param {type: 'string'}\n",
        "exp_path = Path(exp_path)\n",
        "mesh_path = exp_path / \"meshes\"\n",
        "mesh_path = next(mesh_path.glob(\"*.ply\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6BcapUi6ohY"
      },
      "source": [
        "3. レンダリングするフレーム数と視点の角度と距離を入力してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBw6ao10pKVv"
      },
      "outputs": [],
      "source": [
        "#@title { display-mode: \"form\" }\n",
        "#@markdown ←コード実行\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "test_path = dataset_path / \"transforms_test.json.bak\"\n",
        "if not test_path.exists():\n",
        "    test_path = dataset_path / \"transforms_test.json\"\n",
        "    !cp \"{dataset_path}/transforms_test.json\" \"{dataset_path}/transforms_test.json.bak\"\n",
        "\n",
        "with open(test_path, \"r\") as f:\n",
        "    transform = json.load(f)\n",
        "\n",
        "num_frames = 40 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "elev = 30 #@param {type:\"slider\", min:0.0, max:90.0, step:1.0}\n",
        "dist = 4.0 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "trans_t = lambda t : np.array([\n",
        "    [1,0,0,0],\n",
        "    [0,1,0,0],\n",
        "    [0,0,1,t],\n",
        "    [0,0,0,1]])\n",
        "\n",
        "rot_phi = lambda phi : np.array([\n",
        "    [1,0,0,0],\n",
        "    [0,np.cos(phi),-np.sin(phi),0],\n",
        "    [0,np.sin(phi), np.cos(phi),0],\n",
        "    [0,0,0,1]])\n",
        "\n",
        "rot_theta = lambda th : np.array([\n",
        "    [np.cos(th),0,-np.sin(th),0],\n",
        "    [0,1,0,0],\n",
        "    [np.sin(th),0, np.cos(th),0],\n",
        "    [0,0,0,1]])\n",
        "\n",
        "def pose_spherical(theta, phi, radius):\n",
        "    c2w = trans_t(radius)\n",
        "    c2w = rot_phi(phi/180.*np.pi) @ c2w\n",
        "    c2w = rot_theta(theta/180.*np.pi) @ c2w\n",
        "    c2w = np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]]) @ c2w\n",
        "    return c2w\n",
        "\n",
        "c2w = np.stack([pose_spherical(angle, -elev, dist) for angle in np.linspace(-180,180,num_frames+1)[:-1]], 0)\n",
        "c2w = c2w.tolist()\n",
        "\n",
        "frames = []\n",
        "rel_name = transform[\"frames\"][0][\"file_path\"]\n",
        "for i in range(num_frames):\n",
        "    frame = {\n",
        "        \"file_path\": rel_name, \n",
        "        \"transform_matrix\": c2w[i]\n",
        "    }\n",
        "    frames.append(frame)\n",
        "\n",
        "transform[\"frames\"] = frames\n",
        "output_path = dataset_path / \"transforms_test.json\"\n",
        "with open(output_path, \"w\") as outfile:\n",
        "    json.dump(transform, outfile, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZf52xTh7YUk"
      },
      "source": [
        "4. NeRF (torch-ngp) の学習結果をレンダリングします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI9HD4BdytKw"
      },
      "outputs": [],
      "source": [
        "#@title { display-mode: \"form\" }\n",
        "#@markdown ←コード実行\n",
        "\n",
        "scale = 1 #@param {type:\"slider\", min:0.1, max:2, step:0.1}\n",
        "bound = 8 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "bg_radius = 32 #@param {type:\"slider\", min:16, max:64, step:16}\n",
        "\n",
        "!python main_nerf.py \"{dataset_path}\" --workspace \"{exp_path}\" --fp16 --cuda_ray --scale {scale} --bound {bound} --dt_gamma 0.00 --bg_radius {bg_radius} --test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q9lCh--r8TE1"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "def display_mp4(path):\n",
        "    print(f'Read from {path}')\n",
        "    from base64 import b64encode\n",
        "    mp4 = open(path,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display(HTML(\"\"\"\n",
        "    <video controls loop autoplay>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url))\n",
        "    print('Display finished.')\n",
        "\n",
        "display_mp4(\"/content/drive/MyDrive/NeRF-Tutorial/experiments/exp_08302153/results/ngp_ep0338_rgb.mp4\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/Acdg/+afNHOcP3bFd4mp",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}